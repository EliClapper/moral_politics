in 'Simulatie_EliV2.R' Line 53-54:
-Setting a seed in the main R session does not set it in the sessions running on the separate clusters. This makes that results can not replicated. My solution now is passing a vector of seeds the size of the number of rows in the hypergrid to all separate clusters. Setting a seed belonging to the condition within the foreach loop does replicate the results. However, I am wondering, as the conditions become very big, the vector of seeds also becomes very big. Does passing a large vector to all separate clusters impair performance? Every cluster has its own set of registers and are they at risk of overloading?

in 'functions.R' in the simdata() function, I am unsure whether how the data is simulated is correct. Each iteration of the simulation simulates k datasets of 2 variables, of which one is the predictor and the other is the outcome. They are currently generated using mvrnorm() where both variables have a mean of 0 and an sd of 1. The reliability for one specific dataset is simulated by generating a value from a normal distribution with mean = true_es and sd = sqrt(tau2). However, as the true_es gets more extreme (close to 1 or -1) and tau2 gets larger, then the probability of the drawn rho being larger than 1 or smaller than -1 increases. I now solved it with a while loop which can be slow (or even get stuck), is there a better way to implement some variation in the reliability?

in 'functions.R' in the BFs() function, line 29: 
-The sample size for a specific group is drawn from a normal with mean = n and sd = n/3. In line 25, I make sure that the sample size is at least 10. This number is a bit arbitrary and I am wondering if there is a minimum required sample size that can be used instead?

line 68:
- in the bf_together, bain is called once on (r1, r2, .., rk) > hyp_val. Should n be the total sample size combined in all groups?